{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4338158965110779\n",
      "0.43443232774734497\n",
      "0.4357782006263733\n",
      "0.4335229992866516\n",
      "0.4340008795261383\n",
      "0.43050041794776917\n",
      "0.43370741605758667\n",
      "0.43137800693511963\n",
      "0.4331374764442444\n",
      "0.4319044351577759\n",
      "0.43208083510398865\n",
      "0.4325769245624542\n",
      "0.4311603307723999\n",
      "0.4412326514720917\n",
      "0.4372348487377167\n",
      "0.43508797883987427\n",
      "0.43525075912475586\n",
      "0.43109333515167236\n",
      "0.43596985936164856\n",
      "0.4375825524330139\n",
      "0.43016374111175537\n",
      "0.4375734329223633\n",
      "0.4284602403640747\n",
      "0.43513911962509155\n",
      "0.4326404333114624\n",
      "0.43057507276535034\n",
      "0.4286668300628662\n",
      "0.4274650812149048\n",
      "0.4346632957458496\n",
      "0.4342920184135437\n",
      "0.42845550179481506\n",
      "0.4358082115650177\n",
      "0.43508827686309814\n",
      "0.4243760406970978\n",
      "0.43296191096305847\n",
      "0.4322831332683563\n",
      "0.4328440725803375\n",
      "0.43660277128219604\n",
      "0.43391022086143494\n",
      "0.43741780519485474\n",
      "0.4261281490325928\n",
      "0.43422064185142517\n",
      "0.4337971806526184\n",
      "0.43226754665374756\n",
      "0.4312974214553833\n",
      "0.4300386309623718\n",
      "0.43147778511047363\n",
      "0.4324761629104614\n",
      "0.43118104338645935\n",
      "0.42980897426605225\n",
      "0.4301232695579529\n",
      "0.4309452772140503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m out \u001b[38;5;241m=\u001b[39m net(x)\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m L(out, y)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m op\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m op\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/envs/pytorch_env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/pytorch_env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/pytorch_env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "class ResMLP(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = []\n",
    "        input_dim = layers[0]\n",
    "        for dim in layers[1:]:\n",
    "            self.layers.append(nn.Linear(input_dim, dim))\n",
    "            input_dim = dim  \n",
    "            \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # skip block of 1, only works when input dims match output dims\n",
    "            if layer.weight.shape[0] == layer.weight.shape[1]:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "            x = torch.tanh(x)\n",
    "            \n",
    "        return x\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "#net = ResMLP([8 for _ in range(100)])\n",
    "\n",
    "net = []\n",
    "for _ in range(200):\n",
    "    net.append(nn.Linear(128,128))\n",
    "    net.append(nn.ReLU())\n",
    "net = nn.Sequential(*net)\n",
    "op = torch.optim.Adam(net.parameters())\n",
    "L = nn.MSELoss()\n",
    "\n",
    "for _ in range (1000):\n",
    "    x = torch.randn(128, 128)\n",
    "    y = torch.sin(x)\n",
    "    \n",
    "    out = net(x)\n",
    "    loss = L(out, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    op.step()\n",
    "    op.zero_grad()\n",
    "    \n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResMlpLayer(nn.Module):\n",
    "     def __init__(self, dims: int, layers: int):\n",
    "         super().__init__()\n",
    "         self.layers = nn.ModuleList([nn.Linear(dims, dims)for _ in range(layers)])\n",
    "         self.activation = nn.ELU()\n",
    "         \n",
    "     \n",
    "     def forward(self, x):\n",
    "        z = x\n",
    "        for layer in self.layers:\n",
    "            z = layer(z)\n",
    "            z = self.activation(z)\n",
    "        \n",
    "        z = z + x\n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResMLP2(nn.Module):\n",
    "    #h dims is hidden dims\n",
    "    def __init__(self, input_dims:int, h_dims:int, output_dims:int, num_hiden_res_layers:int, res_block_size = 2):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dims, h_dims)\n",
    "        self.activation = nn.ELU()\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            *[ResMlpLayer(h_dims, res_block_size) for _ in range(num_hiden_res_layers)]\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Linear(h_dims, output_dims)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        z = x\n",
    "        z = self.input_layer(z)\n",
    "        z = self.activation(z)\n",
    "        z = self.hidden_layer(z)\n",
    "        z = self.output(z)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
